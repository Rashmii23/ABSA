# -*- coding: utf-8 -*-
"""ATE_latest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16qngcPLDaTGLGY1Mnl096HWT7e0gsWTb
"""

!pip install transformers

# Load and preprocess your dataset
import pandas as pd
dataset = pd.read_csv('input.csv')
dataset

import pandas as pd
dataset = pd.read_csv('input.csv', encoding='utf-8-sig')
# Drop rows with an empty term
dataset = dataset[~dataset['Term'].apply(lambda x: len(eval(x)) == 0)]
dataset.to_csv('input.csv', index=False)

import pandas as pd
from snowballstemmer import stemmer

nepali_stemmer = stemmer("nepali")

def stem_sentence(sentence):
    words = sentence.split()
    stemmed_words = []
    for word in words:
        stemmed_word = nepali_stemmer.stemWord(word)
        stemmed_words.append(stemmed_word)
    stemmed_sentence = " ".join(stemmed_words)
    return stemmed_sentence

# Read the data from the CSV file
df = pd.read_csv("input.csv")

# Apply stemming to the 'Text' column and create a new column 'Text_Stemmed'
df['Text_Stemmed'] = df['Text'].apply(stem_sentence)
df['Text_Stemmed'] = df['Text_Stemmed'].str.replace(',', ' ')
# Save only the 'Text_Stemmed' and 'Term' columns to a new CSV file named "output.csv"
df[['Text_Stemmed', 'Term']].to_csv("input_final.csv", index=False)

df = pd.read_csv('input_final.csv')
df

import csv

def format_data(texts, annotations):
    formatted_data = []
    for text, annotation in zip(texts, annotations):
        tokens = text.split()
        tags = ['0'] * len(tokens)  # Initialize all tags as 'O' (Outside)
        polarities = [-1] * len(tokens)  # Initialize all polarities as -1

        aspect_terms = eval(annotation)  # Parse the annotation string

        if len(aspect_terms) > 0:
            for aspect_term in aspect_terms:
                aspect_text, aspect_sentiment = aspect_term.split(':')
                aspect_text_tokens = aspect_text.split()

                # Find the start index of the aspect term in the tokenized text
                start_index = find_term_index(tokens, aspect_text_tokens, match_type='start')

                # Check if the index is within range
                if start_index is not None and start_index < len(tokens):
                    tags[start_index] = '1'
                    polarity = -1  # Placeholder for unknown polarity
                    if aspect_sentiment == 'positive':
                        polarity = 0
                    elif aspect_sentiment == 'negative':
                        polarity = 1
                    elif aspect_sentiment == 'neutral':
                        polarity = 2
                    polarities[start_index] = polarity
                    for i in range(1, len(aspect_text_tokens)):
                        index = start_index + i
                        if index < len(tokens):
                            if i > 0:
                                tags[index] = '2'
                            polarities[index] = polarity

        # Add the tokenized text, corresponding tags, and polarities to the formatted data
        formatted_data.append((tokens, tags, polarities, aspect_terms))

    return formatted_data

def find_term_index(tokens, term_tokens, match_type='start'):
    token_index = None
    for i in range(len(tokens) - len(term_tokens) + 1):
        if tokens[i:i+len(term_tokens)] == term_tokens:
            token_index = i
            if match_type == 'start':
                break
    return token_index

def read_data_from_csv(file_path):
    texts = []
    annotations = []
    with open(file_path, 'r', encoding='utf-8') as csv_file:
        reader = csv.DictReader(csv_file)
        for row in reader:
            texts.append(row['Text_Stemmed'])
            annotations.append(row['Term'])
    return texts, annotations

def write_data_to_csv(file_path, formatted_data):
    with open(file_path, 'w', encoding='utf-8', newline='') as csv_file:
        fieldnames = ['Text', 'Tags', 'Polarities']
        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
        writer.writeheader()
        for tokens, tags, polarities, aspect_terms in formatted_data:
            text = ' '.join(tokens)
            # Join tags and polarities with a comma
            tags_str = ' '.join(tags)
            polarities_str = ' '.join(map(str, polarities))
            writer.writerow({'Text': text, 'Tags': tags_str, 'Polarities': polarities_str})


input_file = 'input_final.csv'
output_file = 'final1.csv'

texts, annotations = read_data_from_csv(input_file)
formatted_data = format_data(texts, annotations)
write_data_to_csv(output_file, formatted_data)

import pandas as pd
dataset = pd.read_csv('final1.csv', encoding='utf-8-sig')
dataset.head(20)

#drop row if token and tags not equal
import pandas as pd

# Read the CSV file
df = pd.read_csv('final1.csv', encoding='utf-8')

# Filter rows where the number of tags matches the number of tokens
df['Tags'] = df['Tags'].str.split()  # Split the Tags column into lists
df['Token_Count'] = df['Text'].str.split().apply(len)  # Count tokens in the Text column
df = df[df['Tags'].apply(len) == df['Token_Count']]  # Filter rows where Tags and Text token counts match

# Drop the 'Token_Count' column
df = df.drop(columns=['Token_Count'])

# Save the filtered dataset to a new CSV file
df.to_csv('final3.csv', index=False, encoding='utf-8')

dataset = pd.read_csv('final3.csv', encoding='utf-8-sig')
dataset

"""ATE model training"""

import pandas as pd
import torch
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from torch import optim
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader, TensorDataset ,Dataset, RandomSampler, SequentialSampler
from transformers import  BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup, BertForTokenClassification,BertConfig
import time
import numpy as np
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

#use this for dataset conversion to tensor
class dataset_ATM(Dataset):
    def __init__(self, df, tokenizer, max_seq_length):
        self.df = df
        self.tokenizer = tokenizer
        self.max_seq_length = max_seq_length

    def __getitem__(self, idx):
        tokens, tags = self.df.iloc[idx, :2].values
        tokens = tokens.split(' ')
        tags = [int(tag.strip("[]' ").strip()) for tag in tags.split(',')]

        # Tokenize and convert tokens to token IDs
        bert_tokens = self.tokenizer.convert_tokens_to_ids(tokens)

        # Pad or truncate the sequences to the max_seq_length
        if len(bert_tokens) > self.max_seq_length:
            bert_tokens = bert_tokens[:self.max_seq_length]
            tags = tags[:self.max_seq_length]
        elif len(bert_tokens) < self.max_seq_length:
            pad_length = self.max_seq_length - len(bert_tokens)
            bert_tokens += [self.tokenizer.pad_token_id] * pad_length
            tags += [0] * pad_length  # Padding for tags with 0

        ids_tensor = torch.tensor(bert_tokens, dtype=torch.long)
        tags_tensor = torch.tensor(tags, dtype=torch.long)

        # Create masks_tensor
        masks_tensor = (ids_tensor != self.tokenizer.pad_token_id).long()

        return ids_tensor, tags_tensor , masks_tensor

    def __len__(self):
        return len(self.df)

class bert_ATE(torch.nn.Module):
    def __init__(self, pretrain_model):
        super(bert_ATE, self).__init__()
        self.bert = BertModel.from_pretrained(pretrain_model)
        self.linear = torch.nn.Linear(self.bert.config.hidden_size, 3)
        self.loss_fn = torch.nn.CrossEntropyLoss()


    def forward(self, ids_tensors, tags_tensors, masks_tensors):
        bert_outputs,_ = self.bert(input_ids=ids_tensors, attention_mask=masks_tensors,return_dict=False)
        #bert_outputs, _ = outputs.last_hidden_state, outputs.pooler_output

        linear_outputs = self.linear(bert_outputs)
        activated_outputs = self.relu(linear_outputs)


        if tags_tensors is not None:
            tags_tensors = tags_tensors.view(-1)
            linear_outputs = linear_outputs.view(-1, 3)

            loss = self.loss_fn(linear_outputs, tags_tensors)
            return loss
        else:
            return linear_outputs

#practice not using

import torch
from transformers import BertModel
import torch.nn.functional as F

class bert_ATE(torch.nn.Module):
    def __init__(self, pretrain_model):
        super(bert_ATE, self).__init__()
        self.bert = BertModel.from_pretrained(pretrain_model)
        self.linear = torch.nn.Linear(self.bert.config.hidden_size, 3)
        self.loss_fn = torch.nn.CrossEntropyLoss()
        self.relu = torch.nn.ReLU()

    def forward(self, ids_tensors, tags_tensors, masks_tensors):
        bert_outputs,_ = self.bert(input_ids=ids_tensors, attention_mask=masks_tensors, return_dict=False)

        linear_outputs = self.linear(bert_outputs)
        activated_outputs = self.relu(linear_outputs)

        if tags_tensors is not None:
            tags_tensors = tags_tensors.view(-1)
            linear_outputs = linear_outputs.view(-1, 3)

            loss = self.loss_fn(linear_outputs, tags_tensors)
            return loss
        else:
            return activated_outputs  # Changed this line to return activated_outputs when tags_tensors is None

# Load pre-trained BERT model and tokenizer
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
pretrain_model_name = "bert-base-multilingual-uncased"
tokenizer = BertTokenizer.from_pretrained(pretrain_model_name)
lr = 2e-5
model_ATE = bert_ATE(pretrain_model_name).to(DEVICE)
optimizer_ATE = torch.optim.Adam(model_ATE.parameters(), lr=lr)

def evl_time(t):
    min, sec= divmod(t, 60)
    hr, min = divmod(min, 60)
    return int(hr), int(min), int(sec)

def load_model(model, path):
    model.load_state_dict(torch.load(path), strict=False)
    return model

def save_model(model, name):
    torch.save(model.state_dict(), name)

dataset = pd.read_csv('final3.csv', encoding='utf-8-sig')
train_data, temp_data = train_test_split(dataset, test_size=0.2, random_state=42)
valid_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)

train_data_shape = train_data.shape
print("Shape of the train dataset:", train_data_shape)

train_ds = dataset_ATM(train_data, tokenizer,max_seq_length= 60)
test_ds = dataset_ATM(test_data, tokenizer, max_seq_length = 60)

# Iterate through the train_ds dataset directly
for i in range(5):
    ids_tensors, tags_tensors, masks_tensors = train_ds[i]
    # Now you can access and inspect the tensors for each example
    print("Ids Tensors:", ids_tensors)
    print("Tags Tensors:", tags_tensors)
    print("Masks Tensors:", masks_tensors)

def create_mini_batch(samples):
    ids_tensors = [s[0] for s in samples]
    ids_tensors = pad_sequence(ids_tensors, batch_first=True)

    tags_tensors = [s[1] for s in samples]
    tags_tensors = pad_sequence(tags_tensors, batch_first=True)

    masks_tensors = torch.zeros(ids_tensors.shape, dtype=torch.long)
    masks_tensors = masks_tensors.masked_fill(ids_tensors != 0, 1)


    return ids_tensors, tags_tensors, masks_tensors

train_loader = DataLoader(train_ds, batch_size=50, collate_fn=create_mini_batch, shuffle=True)
test_loader = DataLoader(test_ds, batch_size=50, collate_fn=create_mini_batch, shuffle=True)

import matplotlib.pyplot as plt

#Training function
def train_model_ATE(loader, epochs):
    all_data = len(loader)
    train_losses = []
    batch_losses = []

    for epoch in range(epochs):
        finish_data = 0
        losses = []
        current_times = []
        correct_predictions = 0


        for data in loader:
            t0 = time.time()

            ids_tensors, tags_tensors, masks_tensors = data
            ids_tensors = ids_tensors.to(DEVICE)
            tags_tensors = tags_tensors.to(DEVICE)
            masks_tensors = masks_tensors.to(DEVICE)

            loss = model_ATE(ids_tensors=ids_tensors, tags_tensors=tags_tensors, masks_tensors=masks_tensors)
            losses.append(loss.item())
            loss.backward()
            optimizer_ATE.step()
            optimizer_ATE.zero_grad()

            batch_losses.append(loss.item())

            finish_data += 1
            current_times.append(round(time.time()-t0,3))
            current = np.mean(current_times)
            hr, min, sec = evl_time(current*(all_data-finish_data) + current*all_data*(epochs-epoch-1))
            print('epoch:', epoch, " batch:", finish_data, "/" , all_data, " loss:", np.mean(losses), " hr:", hr, " min:", min," sec:", sec)

        # Calculate the training loss for this epoch and append it to the list of training losses
        train_loss = np.mean(losses)
        train_losses.append(train_loss)


    # Plot the training loss graph
    plt.figure(figsize=(10, 5))
    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss Over Epochs')
    plt.legend()
    plt.grid(True)
    plt.show()

    # Plot the loss vs. batch plot
    plt.figure(figsize=(10, 5))
    plt.plot(range(1, len(batch_losses) + 1), batch_losses, label='Loss per Batch')
    plt.xlabel('Batch Iteration')
    plt.ylabel('Loss')
    plt.title('Loss vs. Batch ')
    plt.legend()
    plt.grid(True)
    plt.show()


    save_model(model_ATE, 'bert_ATE.pkl')

#for testing
def test_model_ATE(loader):
    pred = []
    trueth = []
    with torch.no_grad():
        for data in loader:

            ids_tensors, tags_tensors, masks_tensors = data
            ids_tensors = ids_tensors.to(DEVICE)
            tags_tensors = tags_tensors.to(DEVICE)
            masks_tensors = masks_tensors.to(DEVICE)

            outputs = model_ATE(ids_tensors=ids_tensors, tags_tensors=None, masks_tensors=masks_tensors)

            _, predictions = torch.max(outputs, dim=2)

            pred += list([int(j) for i in predictions for j in i ])
            trueth += list([int(j) for i in tags_tensors for j in i ])

    return trueth, pred

# Commented out IPython magic to ensure Python compatibility.
#training epoch = 3
# %time train_model_ATE(train_loader, 3)

# Test the model using the test_loader
x, y = test_model_ATE(test_loader)

# Calculate and print classification metrics
from sklearn.metrics import classification_report

print(classification_report(x, y, target_names=[str(i) for i in range(3)]))

!pip install nepalitokenizers

from nepalitokenizers import WordPiece
import snowballstemmer

class CustomNepaliTokenizer:

    def __init__(self):
        self.stemmer = snowballstemmer.NepaliStemmer()
        self.wordpiece_tokenizer = WordPiece()

    def tokenize_and_stem(self, text):
        # Remove punctuations and split text into words
        punctuations = ['।', ',', ';', '?', '!', '—', '-', '.', '’', '‘']
        for punctuation in punctuations:
            text = text.replace(punctuation, ' ')
        words = text.strip().split()

        # Stem each word and collect the stemmed tokens
        stemmed_tokens = [self.stemmer.stemWord(word) for word in words]

        return stemmed_tokens

    def tokenize_to_ids(self, text):
        tokens = self.wordpiece_tokenizer.encode(text)
        return tokens.ids

    def decode(self, ids):
        return self.wordpiece_tokenizer.decode(ids)

# Example usage:
if __name__ == "__main__":
    tokenizer1= CustomNepaliTokenizer()
    text = "गुडविल फाइनान्सले यस वर्ष लाभांश नदिने"
    tokens = tokenizer1.tokenize_and_stem(text)
    print(tokens)

#for prediction
def predict_model_ATE(sentence, tokenizer):
    word_pieces = []

    tokens = tokenizer1.tokenize_and_stem(text)
    word_pieces += tokens

    ids = tokenizer.convert_tokens_to_ids(word_pieces)
    input_tensor = torch.tensor([ids]).to(DEVICE)

    with torch.no_grad():
        outputs = ATE_model1(input_tensor, None, None)
        _, predictions = torch.max(outputs, dim=2)
    predictions = predictions[0].tolist()

    return word_pieces, predictions, outputs

def ATE_final(text,model):
    terms = []
    word = ""
    pretrain_model_name = "bert-base-multilingual-uncased"
    tokenizer = BertTokenizer.from_pretrained(pretrain_model_name)
    x, y, z = predict_model_ATE(text, tokenizer)
    for i in range(len(y)):
        if y[i] == 1:
            if len(word) != 0:
                terms.append(word.replace(" ##",""))
            word = x[i]
        if y[i] == 2:
            word += (" " + x[i])


    if len(word) != 0:
            terms.append(word.replace(" ##",""))

    print("tokens:", x)
    print("ATE:", terms)

ATE_model1 = load_model(model_ATE, 'bert_ATE.pkl')

text = "पोखरा महानगरपालिकाले उत्पादनका आधारमा ७० प्रतिशत अनुदान दिने"
ATE_final(text,ATE_model1)











"""##Rashmi
practice for prediction using stemmer

"""



from nepalitokenizers import WordPiece
import snowballstemmer

class CustomNepaliTokenizer:

    def __init__(self):
        self.stemmer = snowballstemmer.NepaliStemmer()
        self.wordpiece_tokenizer = WordPiece()

    def tokenize_and_stem(self, text):
        # Remove punctuations and split text into words
        punctuations = ['।', ',', ';', '?', '!', '—', '-', '.', '’', '‘']
        for punctuation in punctuations:
            text = text.replace(punctuation, ' ')
        words = text.strip().split()

        # Stem each word and collect the stemmed tokens
        stemmed_tokens = [self.stemmer.stemWord(word) for word in words]

        return stemmed_tokens

    def tokenize_to_ids(self, text):
        tokens = self.wordpiece_tokenizer.encode(text)
        return tokens.ids

    def decode(self, ids):
        return self.wordpiece_tokenizer.decode(ids)

# Example usage:
if __name__ == "__main__":
    tokenizer1= CustomNepaliTokenizer()
    text = "गुडविल फाइनान्सले यस वर्ष लाभांश नदिने"

    tokens = tokenizer1.tokenize_and_stem(text)
    print(tokens)

    ids = tokenizer1.tokenize_to_ids(text)
    print(ids)

# Function to predict aspect terms in a sentence using stemming

def predict_model_ATE(sentence, tokenizer):
    word_pieces = []
    pretrain_model_name = "bert-base-multilingual-uncased"
    tokenizer = BertTokenizer.from_pretrained(pretrain_model_name)
    tokens = tokenizer1.tokenize_and_stem(text)
    word_pieces += tokens

    ids = tokenizer.convert_tokens_to_ids(word_pieces)
    input_tensor = torch.tensor([ids]).to(DEVICE)

    with torch.no_grad():
        outputs = ATE_model1(input_tensor)
        _, predictions = torch.max(outputs.logits, dim=2)
    predictions = predictions[0].tolist()

    return word_pieces, predictions

# Function to extract aspect terms from predictions

def ATE_final(text):
    terms = []
    word = ""
    x, y = predict_model_ATE(text, tokenizer)
    for i in range(len(y)):
        if y[i] == 1:
            if len(word) != 0:
                terms.append(word.replace(" ##",""))
            word = x[i]
        if y[i] == 2:
            word += (" " + x[i])


    if len(word) != 0:
            terms.append(word.replace(" ##",""))

    print("tokens:", tokens)
    print("ATE:", terms)

ATE_model1 = load_model(model_ATE, 'bert_ATE.pkl')

text = "गुडविल फाइनान्सले यस वर्ष लाभांश नदिने	"
ATE_final(text)





from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')
model_path = 'bert_ATE.pkl'
model_state_dict = torch.load(model_path, map_location=torch.device(DEVICE))
model = bert_ATE(pretrain_model='bert-base-multilingual-uncased')  # Assuming 'bert-base-multilingual-uncased' was used during training
model.load_state_dict(model_state_dict)
model.to(DEVICE)
model.eval()

sentence = "गुडविल फाइनान्सले यस वर्ष लाभांश नदिने"
tokenized_input = tokenizer.encode_plus(
    sentence,
    add_special_tokens=True,
    max_length=128,
    padding='max_length',
    return_attention_mask=True,
    return_tensors='pt'
)

input_ids = tokenized_input['input_ids'].to(DEVICE)
attention_mask = tokenized_input['attention_mask'].to(DEVICE)
masks_tensors = attention_mask;

with torch.no_grad():
    bert_outputs = model(input_ids,attention_mask)

# Assuming you have a separate classification layer on top of BERT for aspect term extraction
token_scores = bert_outputs.logits
predicted_labels = torch.argmax(token_scores, dim=2)

predicted_aspect_terms = []
current_term = []
for token, label in zip(tokenizer.tokenize(sentence), predicted_labels[0].tolist()[0]):
    if label == 1:  # '1' represents the label for aspect terms (you might need to adjust this based on your label scheme)
        current_term.append(token)
    elif label == 0 and current_term:
        predicted_aspect_terms.append(' '.join(current_term))
        current_term = []

# Print the extracted aspect terms
print("Extracted Aspect Terms:", predicted_aspect_terms)